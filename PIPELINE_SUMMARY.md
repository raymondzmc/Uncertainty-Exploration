# Semantic Entropy + AbstentionBench Pipeline

## ‚úÖ Successfully Created Components

### 1. **Main Implementation** (`abstention_semantic_entropy.py`)
- Full integration of semantic entropy with AbstentionBench
- Uses system prompt from AbstentionBench
- Supports 5 datasets: GSM8K, SQuAD2, FalseQA, SelfAware, WorldSense
- Computes semantic entropy via NLI-based clustering
- Evaluates correlation with abstention labels using AUROC

### 2. **Requirements Files**
- `requirements_cpu_macos.txt` - Optimized for macOS (Intel & Apple Silicon)
- `requirements_gpu_ubuntu.txt` - CUDA-optimized for Ubuntu
- Separate configurations for different hardware setups

### 3. **Demo Scripts**
- `demo_complete.py` - Shows the concept with mock data (runs on CPU)
- `demo_concept.py` - Conceptual overview without heavy models
- Both demos successfully demonstrate the pipeline

### 4. **Documentation**
- `README_semantic_abstention.md` - Complete documentation
- `run_semantic_abstention.sh` - Easy execution script

## üìä Demo Results

The `demo_complete.py` successfully shows:
- **Answerable questions** (with context) ‚Üí Lower entropy
- **Unanswerable questions** (context removed) ‚Üí Different entropy patterns
- Semantic clustering of responses
- Abstention detection in generated text

## üöÄ Quick Start

### On CPU (macOS with existing environment)

```bash
# Activate conda environment
conda activate uncertainty

# Install missing dependencies if needed
pip install loguru

# Run conceptual demo (no GPU needed)
python demo_complete.py

# For full pipeline (requires fixing model compatibility)
python abstention_semantic_entropy.py \
    --dataset gsm8k \
    --subset_size 10 \
    --device cpu \
    --model gpt2
```

### Installation from Scratch

```bash
# CPU (macOS)
pip install -r requirements_cpu_macos.txt

# GPU (Ubuntu)
pip install -r requirements_gpu_ubuntu.txt
```

## ‚ö†Ô∏è Known Issues & Fixes Applied

1. **Import Names**: Fixed dataset class names
   - `SQuAD2` ‚Üí `Squad2Dataset`
   - `FalseQA` ‚Üí `FalseQADataset`
   - `SelfAware` ‚Üí `SelfAwareDataset`
   - `WorldSense` ‚Üí `WorldSenseDataset`

2. **Missing Dependencies**: Added `loguru` requirement

3. **Chat Template**: GPT-2 doesn't have chat templates
   - Solution: Use models with chat support (Llama-3.2-1B-Instruct)
   - Or modify code to handle non-chat models

## üî¨ Scientific Validity

The implementation maintains scientific rigor by:
1. Using AbstentionBench's exact system prompt
2. Evaluating on datasets with ground-truth abstention labels
3. Computing proper semantic entropy via bidirectional entailment
4. Measuring performance with standard metrics (AUROC)

## üìà Expected Outcomes

When running with proper models:
- **AUROC > 0.7**: Good correlation between entropy and abstention
- **Higher entropy** for unanswerable questions
- **Lower entropy** for answerable questions

## üéØ Next Steps for Full Deployment

1. **Use compatible models**: 
   ```python
   --model meta-llama/Llama-3.2-1B-Instruct  # Has chat template
   ```

2. **Run on GPU for speed**:
   ```bash
   --device cuda
   ```

3. **Larger evaluation**:
   ```bash
   --subset_size 100  # More examples for statistical significance
   ```

## üìù Citation

This integration combines:
- **Semantic Entropy** (Kuhn et al., ICLR 2023)
- **AbstentionBench** (Kirichenko et al., 2025)

See `README_semantic_abstention.md` for full citations. 